{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised models predicting cubic perovskites\n",
    "\n",
    "In this notebook, we will indulge into different unsupervised models. So far, this includes RandomForest and Gradient Boost. We will be using the library of scikitlearn to do this.\n",
    "\n",
    "The notebook has been modular for further implementation of other machine learning algorithms. It should be relatively straight forward to add more algorithms. \n",
    "\n",
    "Starting off with some basic imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "from src.models import train_model, predict_model\n",
    "from src.features import build_features\n",
    "from src.visualization import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data directory /Users/ohebbi/Documents/UiO/Masterprosjekt/predicting-perovskites-v2/data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Feature selections\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "#metrics and nice visualization\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setting random seed for reproducibility\n",
    "random_state=98127480\n",
    "\n",
    "from pathlib import Path\n",
    "data_dir   = Path.cwd().parent / \"data\" \n",
    "models_dir = Path.cwd().parent / \"models\"\n",
    "print(\"Current data directory {}\".format(data_dir))\n",
    "print(\"Current models directory {}\".format(models_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can read the preprocessed data using pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rA</th>\n",
       "      <th>rB</th>\n",
       "      <th>MA</th>\n",
       "      <th>MB</th>\n",
       "      <th>dAO</th>\n",
       "      <th>dBO</th>\n",
       "      <th>rA/rO</th>\n",
       "      <th>rB/rO</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.460</td>\n",
       "      <td>0.760</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>1.805</td>\n",
       "      <td>2.060</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.460</td>\n",
       "      <td>0.470</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.840</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.348</td>\n",
       "      <td>1.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.460</td>\n",
       "      <td>0.130</td>\n",
       "      <td>65</td>\n",
       "      <td>82</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.096</td>\n",
       "      <td>1.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.460</td>\n",
       "      <td>0.380</td>\n",
       "      <td>65</td>\n",
       "      <td>83</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.604</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.281</td>\n",
       "      <td>1.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.460</td>\n",
       "      <td>0.600</td>\n",
       "      <td>65</td>\n",
       "      <td>85</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.444</td>\n",
       "      <td>1.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>1.196</td>\n",
       "      <td>0.645</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>2.014</td>\n",
       "      <td>1.732</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1.196</td>\n",
       "      <td>0.600</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>2.014</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1.196</td>\n",
       "      <td>0.745</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>2.014</td>\n",
       "      <td>1.849</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1.196</td>\n",
       "      <td>0.670</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>2.014</td>\n",
       "      <td>1.791</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1.196</td>\n",
       "      <td>0.640</td>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>2.014</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rA     rB  MA  MB    dAO    dBO  rA/rO  rB/rO      t\n",
       "0    1.460  0.760  65  86  1.805  2.060  1.081  0.563  0.942\n",
       "1    1.460  0.470  65  95  1.805  1.840  1.081  0.348  1.092\n",
       "2    1.460  0.130  65  82  1.805  1.432  1.081  0.096  1.343\n",
       "3    1.460  0.380  65  83  1.805  1.604  1.081  0.281  1.149\n",
       "4    1.460  0.600  65  85  1.805  1.942  1.081  0.444  1.019\n",
       "..     ...    ...  ..  ..    ...    ...    ...    ...    ...\n",
       "385  1.196  0.645  12  52  2.014  1.732  0.886  0.478  0.902\n",
       "386  1.196  0.600  12  61  2.014  1.750  0.886  0.444  0.923\n",
       "387  1.196  0.745  12  11  2.014  1.849  0.886  0.552  0.859\n",
       "388  1.196  0.670  12  43  2.014  1.791  0.886  0.496  0.891\n",
       "389  1.196  0.640  12  46  2.014  1.749  0.886  0.474  0.905\n",
       "\n",
       "[390 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(data_dir / \"processed\" / \"X.csv\")\n",
    "y = pd.read_csv(data_dir / \"processed\" / \"target.csv\")\n",
    "data = pd.read_csv(data_dir / \"processed\" / \"data.csv\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "Below we define the algorithm to use and its abbreviation. Parameters that are optional to tune are the parameters to the algoriths, with the default value as their optimised value. Another parameter to tune is how many cross-validations one wants to iterate through for the analysis. In addition, one has to find the best features for a new algorithm which will be added further down in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "InsertAlgorithms    = [RandomForestClassifier    (),\\\n",
    "                       GradientBoostingClassifier()]\n",
    "InsertAbbreviations = [\"RF\", \"GB\"]\n",
    "InsertPrettyNames   = [\"Random Forest\", \"Gradient Boost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analysis\n",
    "\n",
    "## Optimal hyperparameters search\n",
    "\n",
    "In this section we will find the optimal parameters used for the various algorithms. We will use imblearn's Pipeline and its implemented samplers, such as SMOTE and RandomUnderSampler. The advantage of using imblearn instead of sklearn, is that sklearn's pipeline will fit the samplers to the validation data as well, while imblearn only fit the resamplers to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TEST\n",
    "\n",
    "#rskfold = RepeatedStratifiedKFold(n_splits=numberSplits, n_repeats=numberRuns, random_state=random_state)\n",
    "\n",
    "#a, grid = applyGridSearch(X = X, y = y.values.reshape(-1,), model=InsertAlgorithms[0], cv = rskfold, sampleMethod=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best params for: RF \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [03:42, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d9e78e85cbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mincludeSampleMethods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finding best params for: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInsertAbbreviations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         bestEstimator, PerovskiteModelsBestParams[InsertAbbreviations[i] + \" \" + method] = train_model.applyGridSearch(\n\u001b[0m\u001b[1;32m     18\u001b[0m                                                                             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UiO/Masterprosjekt/predicting-perovskites-v2/src/models/train_model.py\u001b[0m in \u001b[0;36mapplyGridSearch\u001b[0;34m(X, y, model, cv, sampleMethod)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m#print (grid.best_params_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numberRuns=10\n",
    "numberSplits = 10\n",
    "\n",
    "includeSampleMethods = [\"\", \"under\", \"over\", \"both\"]\n",
    "\n",
    "Abbreviations = []\n",
    "prettyNames   = []\n",
    "Algorithms = []\n",
    "\n",
    "rskfold = RepeatedStratifiedKFold(n_splits=numberSplits, n_repeats=numberRuns, random_state=random_state)\n",
    "\n",
    "PerovskiteModelsBestParams = pd.Series({}, dtype=\"string\")\n",
    "\n",
    "for i, algorithm in tqdm(enumerate(InsertAlgorithms)):\n",
    "    for method in includeSampleMethods:\n",
    "        print(\"Finding best params for: {}\".format(InsertAbbreviations[i] + \" \" + method))\n",
    "        bestEstimator, PerovskiteModelsBestParams[InsertAbbreviations[i] + \" \" + method] = train_model.applyGridSearch(\n",
    "                                                                            X = X, \n",
    "                                                                            y = y.values.reshape(-1,), \n",
    "                                                                        model = algorithm, \n",
    "                                                                           cv = rskfold, \n",
    "                                                                 sampleMethod = method)\n",
    "        Abbreviations.append(InsertAbbreviations[i] + \" \" + method)\n",
    "        prettyNames.append(InsertAbbreviations[i] + \" \" + method)\n",
    "        Algorithms.append(bestEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time cross-validating 100 for learning perovskite structure\n",
    "Under follows the general model runModel that takes the as parameter which model to run and returns nice statistics formatted as a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the different supervised models\n",
    "This is where we generate a lot of different models, and thus will take some time to execute. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerovskiteModels = pd.Series({}, dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, algorithm in enumerate(Algorithms): \n",
    "    print(\"Current training algorithm: {}\".format(prettyNames[i]))\n",
    "    PerovskiteModels[Abbreviations[i]] = (\n",
    "        train_model.runSupervisedModel(classifier  = algorithm, \n",
    "                                     X = X,\n",
    "                                     y = y.values.reshape(-1,),\n",
    "                                     k = numberSplits,\n",
    "                                     n = numberRuns,\n",
    "                      resamplingMethod = \"under\",\n",
    "                     featureImportance = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_accuracy(PerovskiteModels, prettyNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is calculated as a function difference of the 100 models in the purpose of visalizing how much the models deviate from each other.\n",
    "### Interpreting important features used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_important_features(PerovskiteModels, prettyNames, X=X, k = numberSplits, n = numberRuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that random forrest is heavily dependent on four features, while gradient boost is more dependent on three features. We set a limit that the feature needs to be important in at least 50 percent of the classifications.\n",
    "\n",
    "Feature importance is here calculated from the last iteration of the training, and not as mean from the entire dataset. \n",
    "\n",
    "## \"Confusion\" metrics\n",
    "\n",
    "### Sorting as a function of alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_confusion_metrics(PerovskiteModels, prettyNames, data, k = numberSplits, n = numberRuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots we see that gradient boost is more likely to misclassify a wider variation of compounds, with an exception for MgSiO3 for false negatives. Random forest, on the other hand, is more likely to misclassify the same compounds for all models.  \n",
    "### Sorting as a function of tolerance factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure( \n",
    "    layout = go.Layout (\n",
    "        title=go.layout.Title(text=\"False positives (Nruns = {})\".format(numberSplits*numberRuns)),\n",
    "        yaxis=dict(title='Counts'),\n",
    "        xaxis=dict(title='tolerance factor')\n",
    "        )\n",
    "    )\n",
    "\n",
    "for i, model in enumerate(PerovskiteModels):\n",
    "    fig.add_traces(go.Bar(name=prettyNames[i], \n",
    "                          x=data['t'][model['falsePositives'] > 0],\n",
    "                          y=model['falsePositives'][model['falsePositives'] > 0],\n",
    "                          text=data['Compound'][model['falsePositives'] > 0],\n",
    "                          )\n",
    "                  )\n",
    "\n",
    "fig.update_layout(barmode='group')\n",
    "fig.add_trace(go.Scatter(x=[0.73,0.75], y=[5,5], fill='tozeroy',\n",
    "                    mode= 'none', name='Not a perovskite'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0.75,0.9], y=[10,10], fill='tozeroy',\n",
    "                    mode= 'none', name='Orthorombic perovskite'))\n",
    "fig.add_trace(go.Scatter(x=[0.9,1.0], y=[10,10], fill='tozeroy',\n",
    "                    mode= 'none', name='Cubic perovskite'))\n",
    "fig.add_trace(go.Scatter(x=[1.0,1.15], y=[5,5], fill='tozeroy',\n",
    "                    mode= 'none', name='Hexagonal nonperovskite'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the false positive plot above, we see that there are several compounds with the same tolerance factor, giving rise to several more counts per tolerance factor compared to what is actually shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure( \n",
    "    layout = go.Layout (\n",
    "        title=go.layout.Title(text=\"falseNegatives (Nruns = {})\".format(numberSplits*numberRuns)),\n",
    "        yaxis=dict(title='Counts'),\n",
    "        xaxis=dict(title='tolerance factor')\n",
    "        )\n",
    "    )\n",
    "\n",
    "for i, model in enumerate(PerovskiteModels):\n",
    "    fig.add_traces(go.Bar(name=prettyNames[i], \n",
    "                          x=data['t'][model['falseNegatives'] > 0],\n",
    "                          y=model['falseNegatives'][model['falseNegatives'] > 0],\n",
    "                          text=data['Compound'][model['falseNegatives'] > 0],\n",
    "                          )\n",
    "                  ) \n",
    "fig.update_layout(barmode='group')\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0.73,0.75], y=[5,5], fill='tozeroy',\n",
    "                    mode= 'none', name='Not a perovskite'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0.75,0.9], y=[10,10], fill='tozeroy',\n",
    "                    mode= 'none', name='Orthorombic perovskite'))\n",
    "fig.add_trace(go.Scatter(x=[0.9,1.0], y=[10,10], fill='tozeroy',\n",
    "                    mode= 'none', name='Cubic perovskite'))\n",
    "fig.add_trace(go.Scatter(x=[1.0,1.15], y=[5,5], fill='tozeroy',\n",
    "                    mode= 'none', name='Hexagonal nonperovskite'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure( \n",
    "    layout = go.Layout (\n",
    "        title=go.layout.Title(text=\"falseNegatives (Nruns = {})\".format(numberRuns*numberSplits)),\n",
    "        yaxis=dict(title='Counts'),\n",
    "        xaxis=dict(title='MA')\n",
    "        )\n",
    "    )\n",
    "\n",
    "for i, model in enumerate(PerovskiteModels):\n",
    "    fig.add_traces(go.Bar(name=prettyNames[i], \n",
    "                          x=data['MA'][model['falseNegatives'] > 0],\n",
    "                          y=model['falseNegatives'][model['falseNegatives'] > 0],\n",
    "                          text=data['Compound'][model['falseNegatives'] > 0],\n",
    "                          )\n",
    "                  ) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure( \n",
    "    layout = go.Layout (\n",
    "        title=go.layout.Title(text=\"falseNegatives (Nruns = {})\".format(numberRuns*numberSplits)),\n",
    "        yaxis=dict(title='Counts'),\n",
    "        xaxis=dict(title='MB')\n",
    "        )\n",
    "    )\n",
    "\n",
    "for i, model in enumerate(PerovskiteModels):\n",
    "    fig.add_traces(go.Bar(name=prettyNames[i], \n",
    "                          x=data['MB'][model['falseNegatives'] > 0],\n",
    "                          y=model['falseNegatives'][model['falseNegatives'] > 0],\n",
    "                          text=data['Compound'][model['falseNegatives'] > 0],\n",
    "                          )\n",
    "                  ) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which perovskites are correctly predicted?\n",
    "\n",
    "Out of 100 different runs, the article yield a prediction if over 50% of the predictions are in favor for either perovskite or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_confusion_matrix(PerovskiteModels, y, data, abbreviations=Abbreviations, names=prettyNames, k = numberSplits, n = numberRuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can choose whichever confidence we would like to make a confusion matrix. The article has used a binomial distribution, without doing the statistics behind a one-side hypothesis test. Let us do better. \n",
    "\n",
    "We will start by excluding every compound that has been misclassified over 50% of the classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.confusion_matrix_plot(PerovskiteModels, y, prettyNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data for the cubic classification\n",
    "\n",
    "After training a model on the perovskite data, we would like to make a smaller subset of the dataset as the training data for a model that can classify a cubic perovskite versus a non-cubic perovskite. We will remove all compounds that got misclassified at least 50% of 100 classifications, in addition to non-perovskites. Thus, if two algorithms is seen with same compounds that are being removed due to false negatives but different number of false positives, they will have the same test data.\n",
    "\n",
    "To start of, which compounds are predicted most times as perovskite, but are in fact non-perovskite? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(PerovskiteModels):\n",
    "    print(prettyNames[i], \": over 50 percent misclassifications:\")\n",
    "    print(\"False negatives:\")\n",
    "    print(data['Compound'][model['falseNegatives'] > numberRuns*numberSplits/2])\n",
    "    print(\"False positives:\")\n",
    "    print(data['Compound'][model['falsePositives'] > numberRuns*numberSplits/2])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowedOfNumberMiscalculations = 50\n",
    "correctlyPredictedPerovskites = {}\n",
    "\n",
    "for abbreviation in Abbreviations:     \n",
    "    correctlyPredictedPerovskites[abbreviation] = visualize.findCorrectlyPredictedPerovskites(\n",
    "        PerovskiteModels[abbreviation], data, allowedOfNumberMiscalculations)\n",
    "\n",
    "correctlyPredictedPerovskites[\"GB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models \n",
    "\n",
    "Now, we will save the perovskite-models for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for i, algorithm in tqdm(enumerate(Algorithms)):\n",
    "    file_path = Path(models_dir / \"perovskite\" / Path(prettyNames[i] + \".pkl\"))\n",
    "    with file_path.open('wb') as fp:\n",
    "        pickle.dump(algorithm, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correctlyPredictedPerovskites[\"RF \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model for cubic perovskite or not\n",
    "\n",
    "## Finding the optimal hyperparameters imblearn\n",
    "\n",
    "We will here do the same procedure as for the perovskite model, however, with the added complexity that the different models will run on different data depending on what the model predicted for perovskites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskfold = RepeatedStratifiedKFold(n_splits=numberSplits, n_repeats=numberRuns, random_state=random_state)\n",
    "allFeatures = [\"rA\", \"rB\", \"MA\", \"MB\", \"dAO\", \"dBO\", \"rA/rO\", \"rB/rO\", \"t\"]\n",
    "CubicModelsBestParams = pd.Series({}, dtype=\"string\")\n",
    "BestParamsAlgorithms = []\n",
    "for i, algorithm in tqdm(enumerate(InsertAlgorithms)):\n",
    "    for method in includeSampleMethods:\n",
    "        print(\"Finding best params for: {}\".format(InsertAbbreviations[i] + \" \" + method))\n",
    "        bestEstimator, CubicModelsBestParams[InsertAbbreviations[i] + \" \" + method] = = train_model.applyGridSearch(\n",
    "                                                                            X = correctlyPredictedPerovskites[InsertAbbreviations[i] + \" \" + method][allFeatures],\n",
    "                                                                            y = correctlyPredictedPerovskites[InsertAbbreviations[i] + \" \" + method][\"Cubic\"].values.reshape(-1,),\n",
    "                                                                        model = algorithm, \n",
    "                                                                           cv = rskfold, \n",
    "                                                                 sampleMethod = method)\n",
    "        # Only the parameters for the algorithm changes here, \n",
    "        # since pretty name and abbreviations are already done.\n",
    "        BestParamsAlgorithms.append(bestEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CubicModels = pd.Series({}, dtype=\"string\")\n",
    "for i, algorithm in enumerate(BestParamsAlgorithms): \n",
    "    print(\"Current training algorithm: {}\".format(prettyNames[i]))\n",
    "    CubicModels[Abbreviations[i]] = (\n",
    "        train_model.runSupervisedModel(classifier  = algorithm, \n",
    "                                     X = correctlyPredictedPerovskites[Abbreviations[i]][allFeatures],\n",
    "                                     y = correctlyPredictedPerovskites[Abbreviations[i]][\"Cubic\"].values.reshape(-1,),\n",
    "                                     k = numberSplits,\n",
    "                                     n = numberRuns,\n",
    "                      resamplingMethod = \"None\",\n",
    "                     featureImportance = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_accuracy(CubicModels, prettyNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_important_features(CubicModels, prettyNames, X, k = numberSplits, n = numberRuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(CubicModels):\n",
    "    print(X.columns[model[\"importantKeys\"]>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see here that some of the features are redundant, as both random forrest and gradient boost agrees on rA, rA/rO and t as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_confusion_metrics(CubicModels, \n",
    "                                 prettyNames, \n",
    "                                 correctlyPredictedPerovskites, \n",
    "                                 k = numberSplits, \n",
    "                                 n = numberRuns, \n",
    "                                 abbreviations = Abbreviations, \n",
    "                                 cubicCase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_confusion_matrix(CubicModels,\\\n",
    "                      correctlyPredictedPerovskites,\\\n",
    "                      correctlyPredictedPerovskites,\n",
    "                      abbreviations = Abbreviations,\n",
    "                      names = prettyNames,\n",
    "                      k = numberSplits,\n",
    "                      n = numberRuns,\n",
    "                      cubicCase = True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the test set\n",
    "\n",
    "We will be using the generated training datasets for both the perovskite and cubic classification to train a model, which will be used to classify a completely independent test dataset. \n",
    "\n",
    "## Predicting perovskites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PerovskiteTestData = pd.read_csv(data_dir / \"processed\" / \"625TestData.csv\")\n",
    "\n",
    "Summary             = pd.DataFrame({}, dtype=\"string\")\n",
    "Summary[\"Compound\"] = PerovskiteTestData[\"Compound\"]\n",
    "\n",
    "PredictedPerovskites = pd.Series({}, dtype=\"string\")\n",
    "\n",
    "threshold = numberSplits*numberRuns/2 #50% when equal\n",
    "\n",
    "for i, algorithm in enumerate(Algorithms):#\n",
    "    PredictedPerovskites[Abbreviations[i]+\" P\"], PredictedPerovskites[Abbreviations[i]+\" P Prob\"] = predict_model.runPredictions(algorithm,\\\n",
    "                                    trainingData   = X[X.columns[PerovskiteModels[Abbreviations[i]][\"importantKeys\"]>threshold]],\\\n",
    "                                    trainingTarget = y.values.reshape(-1,),\\\n",
    "                                    testData   = PerovskiteTestData[PerovskiteTestData[allFeatures].columns[PerovskiteModels[Abbreviations[i]][\"importantKeys\"]>threshold]])\n",
    "\n",
    "for abbreviation in Abbreviations:\n",
    "    PerovskiteTestData[abbreviation + \" P\"] = PredictedPerovskites[abbreviation + \" P\"]\n",
    "    Summary[abbreviation + \" P\"]            = PredictedPerovskites[abbreviation + \" P\"]\n",
    "    Summary[abbreviation + \" P Prob\"]       = PredictedPerovskites[abbreviation + \" P Prob\"]\n",
    "    print(abbreviation, \"predict the number of perovskites as: \", np.sum(PredictedPerovskites[abbreviation+\" P\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding training and test data for the cubic prediction\n",
    "Now, we need to remove all the non-perovskite in order to classify them into cubic-perovskites or non-cubic perovskites. Alas, the results above does imply many different training sets because of different results from the classifiers from the previous classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CubicTestData = pd.Series({}, dtype=\"string\")\n",
    "\n",
    "def RemoveNonPerovskites(predictions):\n",
    "    remove_indices = np.array(PerovskiteTestData['Compound'][predictions == 0].index)\n",
    "    cubicTestData = PerovskiteTestData.drop(index=remove_indices)\n",
    "    cubicTestData.reset_index(drop=False, inplace=True)\n",
    "    return cubicTestData\n",
    "\n",
    "for abbreviation in Abbreviations:\n",
    "    CubicTestData[abbreviation] = RemoveNonPerovskites(PerovskiteTestData[abbreviation+\" P\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CubicTestData[\"RF \"].shape)\n",
    "print(CubicTestData[\"GB \"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, here we meet a challenge since the training data does only contain less than 9 percent cubic perovskites. When training the model earlier, we used a stratified cross fold validation scheme to create training and validation sets on the basis of a 50/50 split. We did this by the help of a stratified parameter which is built into SciKitLearn. Now, we would like to use all of the cubic data while keeping it 50/50 on the entire dataset, but this we need to make ourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#correctlyPredictedPerovskites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not using pandas dataframe because the contents does not neccessarily have the same shape\n",
    "stratifiedCubicData = pd.Series({}, dtype=\"string\")\n",
    "\n",
    "#choose percentage of data that should be cubic. 50/50 split is percentage=0.085\n",
    "percentage = 0.25\n",
    "\n",
    "def getStratifiedTrainingData(predictedPerovskites, Name):\n",
    "    cubics    = predictedPerovskites.iloc[predictedPerovskites[\"Cubic\"][predictedPerovskites[\"Cubic\"]==1].index]\n",
    "    nonCubics = predictedPerovskites.iloc[predictedPerovskites[\"Cubic\"][predictedPerovskites[\"Cubic\"]!=1].index]\n",
    "    print(Name, \":\")\n",
    "    print(\"The amount of cubic perovskites entries in the data is {}, with a total percentage of {}\"\\\n",
    "          .format(np.sum(cubics[\"Cubic\"]), np.sum(cubics[\"Cubic\"])/len(predictedPerovskites[\"Cubic\"])))\n",
    "    \n",
    "    # The data trained on should be evenly distributed. Here, we are just picking random numbers. \n",
    "    nonCubicsSubSet = nonCubics.sample(n = int(percentage*len(predictedPerovskites.index)), random_state=random_state)\n",
    "\n",
    "    #test to make the reader aware of the distribution in the training data\n",
    "    if (nonCubicsSubSet.shape!=cubics.shape):\n",
    "        print(\"Current shape Cubics: {} and nonCubics: {}\".format(cubics.shape, nonCubicsSubSet.shape))\n",
    "    \n",
    "    #Combining the subsets\n",
    "    stratCubicData = pd.concat([cubics, nonCubicsSubSet])\n",
    "    stratCubicData.reset_index(drop=False, inplace=True)\n",
    "    return stratCubicData\n",
    "\n",
    "for abbreviation in Abbreviations: \n",
    "    print(\"Name  :{}\".format(abbreviation))\n",
    "    print(\"Shape :{}\".format(correctlyPredictedPerovskites[abbreviation].shape))\n",
    "    stratifiedCubicData[abbreviation] = getStratifiedTrainingData(correctlyPredictedPerovskites[abbreviation], abbreviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting cubic perovskites\n",
    "Now that we have stratified data, we can finally fit our classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(stratifiedCubicData):\n",
    "    print(\"The shape of {} is: {}\".format(prettyNames[i], data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictedCubics = {}\n",
    "for i, algorithm in enumerate(Algorithms):\n",
    "    PredictedCubics[Abbreviations[i]+\" C\"], PredictedCubics[Abbreviations[i]+\" C Prob\"] = predict_model.runPredictions(algorithm,\\\n",
    "                                    trainingData   = stratifiedCubicData[Abbreviations[i]][stratifiedCubicData[Abbreviations[i]][allFeatures].columns[CubicModels[Abbreviations[i]][\"importantKeys\"]>threshold]],\\\n",
    "                                    trainingTarget = stratifiedCubicData[Abbreviations[i]][\"Cubic\"],\\\n",
    "                                    testData   = CubicTestData[Abbreviations[i]][CubicTestData[Abbreviations[i]][allFeatures].columns[CubicModels[Abbreviations[i]][\"importantKeys\"]>threshold]])\n",
    "    print(Abbreviations[i], \" predict the number of cubic perovskites as: \", np.sum(PredictedCubics[Abbreviations[i]+\" C\"]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for abbreviation in Abbreviations:\n",
    "    tmp1 = np.empty(len(Summary.index))\n",
    "    tmp2 = np.copy(tmp1)\n",
    "    tmp2[:] = np.nan\n",
    "    tmp1[:] = -1 #For all non-perovskites\n",
    "\n",
    "    tmp1[CubicTestData[abbreviation][\"index\"][PredictedCubics[abbreviation + \" C\"] == 1].values] = 1\n",
    "    tmp1[CubicTestData[abbreviation][\"index\"][PredictedCubics[abbreviation + \" C\"] == 0].values] = 0\n",
    "\n",
    "    tmp2[CubicTestData[abbreviation][\"index\"].values] = PredictedCubics[abbreviation + \" C Prob\"]\n",
    "\n",
    "    Summary[abbreviation + \" C\"]       = tmp1.astype(int)\n",
    "    Summary[abbreviation + \" C Prob\"]  = tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary[Summary[\"GB  C\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see our candidates with their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary[Summary[\"RF C\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary.to_csv(data_dir / \"summary\" / \"625SupervisedPredictions.csv\", sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
